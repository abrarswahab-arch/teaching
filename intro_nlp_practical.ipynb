{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwCUw45mZKjG"
      },
      "source": [
        "Intro to NLP Practical<br>\n",
        "======================<br>\n",
        "Students will work through problems on n-grams, probabilities, OOV handling, and classifiers.<br>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "helllooooo"
      ],
      "metadata": {
        "id": "DUyYnIHcC8Dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hJnSQ_vqZKjO"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2tr5e7WZKjQ"
      },
      "source": [
        "Toy corpus for language modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0uJmCM0sZKjS"
      },
      "outputs": [],
      "source": [
        "corpus1 = [\n",
        "    \"Mary had a little lamb\",\n",
        "    \"Its fleece was white as snow\",\n",
        "    \"And everywhere that Mary went\",\n",
        "    \"The lamb was sure to go\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nIvCzEeZKjT"
      },
      "source": [
        "--- Part 1: Preprocessing ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re-DkrvQZKjU"
      },
      "source": [
        " Q1.1 Sequence notation<br>\n",
        "Exercise: Write sequence notation for the sentence:<br>\n",
        "\"Mary had a little lamb, its fleece was white as snow\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\"Mary had a little lamb\", \"Its fleece was white as snow\"]"
      ],
      "metadata": {
        "id": "s5Kj8GWGF_bC"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words = 100)\n"
      ],
      "metadata": {
        "id": "QDYGrC6EFI7R"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAOpzABuZKjV"
      },
      "source": [
        " Q1.2 Add start/end tokens<br>\n",
        "Exercise: Write a function to tokenize the corpus and add <s>, </s>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "print(\"Word Index: \", word_index)\n",
        "print(\"Sequences: \", sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kH5wqrIFq5V",
        "outputId": "4abdb795-1673-400a-f736-05b39cd6f279"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Index:  {'mary': 1, 'had': 2, 'a': 3, 'little': 4, 'lamb': 5, 'its': 6, 'fleece': 7, 'was': 8, 'white': 9, 'as': 10, 'snow': 11}\n",
            "Sequences:  [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10, 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_start_ending(corpus):\n",
        "  tokenized_corpus = []\n",
        "  for sentence in corpus:\n",
        "    tokenized_sentences = [\"<s>\"] + sentence.split() + [\"</s>\"]\n",
        "    tokenized_corpus.append(tokenized_sentences)\n",
        "  return tokenized_corpus\n",
        "tokenized_corpus=add_start_ending(corpus)\n",
        "print(tokenized_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVeVAqZOHV4l",
        "outputId": "d3ff1910-c449-4999-d8e1-fe9f947645e0"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['<s>', 'Mary', 'had', 'a', 'little', 'lamb', '</s>'], ['<s>', 'Its', 'fleece', 'was', 'white', 'as', 'snow', '</s>']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRH1mpV5ZKjY"
      },
      "source": [
        "--- Part 2: N-grams & Probabilities ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xVR2UlsZKjZ"
      },
      "source": [
        " Q2.1 Extract unigrams, bigrams, trigrams"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6aXgc-VMky4",
        "outputId": "f92ba9f3-298c-4c85-ab84-4d95e53e7577"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7PJ3mWdM0T8",
        "outputId": "eb7dced8-b968-4dc2-832f-1e243cf48d49"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bigram_probability(bigrams):\n",
        "  bigram_counts = Counter(bigrams)\n",
        "  unigram_counts = Counter([bigram[0] for bigram in bigrams])\n",
        "  probabilities = {}\n",
        "  for (w1, w2), count in bigram_counts.items():\n",
        "    probabilities[(w1, w2)] = count / unigram_counts[w1]\n",
        "  return probabilities\n",
        "\n",
        "# The print statements were outside the function, I've moved them here\n",
        "# and fixed the f-string\n",
        "bigram_probabilities = bigram_probability(bigrams) # Assuming 'bigrams' is defined\n",
        "\n",
        "print(\"\\nBigram Probabilities:\")\n",
        "for (w1, w2), prob in bigram_probabilities.items():\n",
        "  print(f\"{w1} {w2}: {prob}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbtKpmEnLdi6",
        "outputId": "898f8160-051e-441c-dc0c-fdf1012cbb8e"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bigram Probabilities:\n",
            "Mary had: 1.0\n",
            "had a: 0.5\n",
            "a little: 1.0\n",
            "little lamb: 1.0\n",
            "Its had: 1.0\n",
            "had was: 0.5\n",
            "was white: 1.0\n",
            "white as: 1.0\n",
            "as snow: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "\n",
        "all_bigrams = []\n",
        "for sentence in corpus:\n",
        "    tokens = word_tokenize(sentence)\n",
        "    bigrams = list(ngrams(tokens, 2))\n",
        "    all_bigrams.extend(bigrams)\n",
        "\n",
        "# Now 'all_bigrams' contains bigrams from all sentences\n",
        "# You can use 'all_bigrams' in the next cell\n",
        "bigrams = all_bigrams # Assign to 'bigrams' to match the next cell's expectation"
      ],
      "metadata": {
        "id": "22GPGk8oMp4t"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RuQzfXWZKjb"
      },
      "source": [
        " Q2.2 Bigram probabilities<br>\n",
        "Exercise: Write function to compute P(w_i | w_{i-1})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_probabilities = bigram_probability(bigrams) # Assuming 'bigrams' is defined\n",
        "print(\"\\nBigram Probabilities:\")\n",
        "for (w1, w2), prob in bigram_probabilities.items():\n",
        "  print(f\"{w1} {w2}: {prob}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkAbBsytPGMZ",
        "outputId": "d0d29214-17b7-48bd-ca43-71c4fb2e2502"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bigram Probabilities:\n",
            "Mary had: 1.0\n",
            "had a: 1.0\n",
            "a little: 1.0\n",
            "little lamb: 1.0\n",
            "Its fleece: 1.0\n",
            "fleece was: 1.0\n",
            "was white: 1.0\n",
            "white as: 1.0\n",
            "as snow: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3G1JGIAZKjj"
      },
      "source": [
        " Q2.3 Sentence probability<br>\n",
        "Exercise: Compute probability of \"Mary had a little lamb\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_probability(sentence, bigram_probabilities):\n",
        "  tokens = word_tokenize(sentence)\n",
        "  bigrams = list(ngrams(tokens, 2))\n",
        "  prob = 1.0\n",
        "  for bigram in bigrams:\n",
        "    w1, w2 = bigram\n",
        "    if (w1, w2) in bigram_probabilities:\n",
        "      prob *= bigram_probabilities[(w1, w2)]\n",
        "    # else:\n",
        "    #   prob = 0.0\n",
        "    #   break\n",
        "  return prob\n",
        "sentence = \"Mary had a little lamb\"\n",
        "probability = sentence_probability(sentence, bigram_probabilities)\n",
        "print(f\"Probability of '{sentence}': {probability}\")"
      ],
      "metadata": {
        "id": "IsMAOgm_TZ6Z",
        "outputId": "9eea08b0-fe96-412c-8fbb-3eac7b51cfb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability of 'Mary had a little lamb': 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0unZilSZKjk"
      },
      "source": [
        "Q2.4 Handling OOV/UNK<br>\n",
        "Exercise: Replace unseen words with <UNK> and recompute\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evU5bff_ZKjl"
      },
      "source": [
        "--- Part 3: Classifier ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3DxxnAKZKjm"
      },
      "source": [
        " Q3.1 Naive Bayes sentiment classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📽 Exercise 3.1: Sentiment Classification on toy dataset\n",
        "\n",
        "In this exercise, you will build a simple sentiment classification model that predicts whether a given sentence is **positive** or **negative**.\n",
        "\n",
        "---\n",
        "\n",
        "## ✏️ Instructions:\n",
        "\n",
        "\n",
        "### 1️⃣ Perform Feature Extraction\n",
        "- Use **TF-IDF Vectorization** to convert names into numerical features.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 2️⃣ Train a Machine Learning Classifier\n",
        "- Use any classifier you are familiar with (e.g., **Logistic Regression** or **Naive Bayes**).\n",
        "- Split the data into **training** and **testing** sets.\n",
        "- Train the classifier on the training data.\n",
        "\n",
        "\n",
        "🚀 **Goal:** By the end of this exercise, you should be able to:\n",
        "- Apply **feature extraction** to text data.\n",
        "- Train and evaluate a **text classification model** using **machine learning**."
      ],
      "metadata": {
        "id": "Yyx3z1RL58el"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3fkQ5DUZKjm"
      },
      "outputs": [],
      "source": [
        "train_texts = [\n",
        "    \"I love my dog\",\n",
        "    \"This food is great\",\n",
        "    \"I hate waiting\",\n",
        "    \"The movie was boring\",\n",
        "    \"Happy with my phone\",\n",
        "    \"This is awful\"\n",
        "]\n",
        "train_labels = [\"pos\", \"pos\", \"neg\", \"neg\", \"pos\", \"neg\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📽 Exercise 3.2: Movie Review Classification using Movies Review Corpus\n",
        "\n",
        "In this exercise, you will build a simple text classification model that predicts whether a given **movie review** is **positive** or **negative** using the **NLTK Movie Reviews Corpus**.\n",
        "\n",
        "This is a classical example of text classification at the **sentence level**.\n",
        "\n",
        "---\n",
        "\n",
        "## ✏️ Instructions:\n",
        "\n",
        "### 1️⃣ Load the Data\n",
        "- Import the **Movie Reviews corpus** from **NLTK**.\n",
        "- Create a dataset where each example is a review and the label is either `'positive'` or `'negative'`.\n",
        "\n",
        "---\n",
        "\n",
        "### 2️⃣ Perform Feature Extraction\n",
        "- Use **TF-IDF Vectorization** to convert names into numerical features.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 3️⃣ Train a Machine Learning Classifier\n",
        "- Use any classifier you are familiar with (e.g., **Logistic Regression** or **Naive Bayes**).\n",
        "- Split the data into **training** and **testing** sets.\n",
        "- Train the classifier on the training data.\n",
        "\n",
        "---\n",
        "\n",
        "### 4️⃣ Evaluate the Classifier\n",
        "- Use **accuracy** and a **classification report** to evaluate your model on the test set.\n",
        "- Think about: How well does the model perform? Which reviews are harder to classify?\n",
        "\n",
        "---\n",
        "\n",
        "✅ You are free to explore:\n",
        "- Trying different classifiers.\n",
        "- Visualizing the results (e.g., confusion matrix).\n",
        "\n",
        "---\n",
        "\n",
        "🚀 **Goal:** By the end of this exercise, you should be able to:\n",
        "- Apply **feature extraction** to text data.\n",
        "- Train and evaluate a **text classification model** using **machine learning**."
      ],
      "metadata": {
        "id": "fq7Vj1ng1lw8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVYgyaWtZKju"
      },
      "source": [
        " Q3.3 Discussion: Why bigrams vs unigrams?<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xv13VAlzZKju"
      },
      "source": [
        " Q3.4 Limitations of n-grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mIB4S3wZKju"
      },
      "source": [
        "--- Part 4: Wrap-up Reflection ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzsTXZsTZKjv"
      },
      "source": [
        " Discussion Questions<br>\n",
        "1. Why do we need <UNK> tokens?<br>\n",
        "2. Why start/end tokens?<br>\n",
        "3. Why not always use higher n-grams?<br>\n",
        "4. How do classifiers differ from language models?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}